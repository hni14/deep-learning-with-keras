# ニューラルネットワークを設計する

## 概要
[パーセプトロンで論理回路を作成する](../perceptron) を通じて、パーセプトロンの仕組みを学びました。  
また、パーセプトロンがニューラルネットワークの一種であることを学びました。  
この例では、ニューラルネットワークの設計について学んでいきます。  

この例題を通じて、以下の理解を目指します。  

- 解くべき問題に応じて、ニューラルネットワークの静的構造を設計する必要がある。
- ニューラルネットワークは、入力層、隠れ層、出力層で構成されている。
- 入力層の設計においては、入力層におけるノード数を決定する必要がある。
- 隠れ層の設計においては、隠れ層の数、各隠れ層におけるノードの数、各ノードの重み・バイアス、各隠れ層における活性化関数を決める必要がある。
- 隠れ層の活性化関数としては、なめらかに変化する関数を使用する。
- 出力層の設計においては、出力層におけるノードの数、各ノードの重み、バイアス、出力層における活性化関数を決める必要がある。
- 出力層の活性化関数としては、多クラス分類問題ならソフトマックス(softmax)、２値分類問題ならなめらかな関数(sigmoid等)、回帰問題なら恒等関数(linear)を用いる。

### ニューラルネットワークの構造
ニューラルネットワークは、入力層、隠れ層、出力層から構成されます。  
また、各層はN個のノード(ニューロンとも呼ばれます)により構成されます。  

[パーセプトロンで論理回路を作成する](../perceptron)で扱った、XORゲートのネットワーク構造を見てみましょう。  
この図の一番左の列を入力層、中間の列を隠れ層、一番右の列を出力層と呼びます。  
XORゲートのようなネットワーク構造をもつニューラルネットワークは、「2層ネットワーク」と呼ばれます。  
重みやバイアスを持つノードから構成される層は隠れ層と出力層のみのため、「2層」と呼ばれます。  
入力層には2つのノード、隠れ層には2つのノード、出力層には1つのノードが存在します。  

#### XORゲート
![XORゲート](../perceptron/img/xor_gate.dot.png)

#### XORゲートの構成
|論理演算|x1の重み|x2の重み|バイアス|活性化関数  |対応層|
|--------|--------|--------|--------|------------|------|
|NAND    |-0.5    |-0.5    | 0.7    |ステップ関数|隠れ層|
|OR      | 0.5    | 0.5    |-0.2    |ステップ関数|隠れ層|
|AND     | 0.5    | 0.5    |-0.7    |ステップ関数|出力層|


### 入力層の設計
入力層とは、ニューラルネットワークへの入力信号をまとめた層であり、この層では、重みやバイアスを持つノードは存在しません※。  
※各入力値を１つずつ受ける重みが1、バイアスが0、かつ、加重和をそのまま出力する活性化関数を持つ層と解釈することはできます。  
ニューラルネットワークにおいては、必ず入力層が必要であり、全結合のネットワークの場合、層の数は必ず１つです。  

#### 入力層のノード数の決定
入力層においては、ノード数を決める必要があります。  
しかし、これは訓練用データからある程度自動で決まります。  
例えば、論理ゲートでは、入力はx1, x2のみで、入力層におけるノードの数は２つになります。  

##### XOR 真理値表
| x1 | x2 | y |
|----|----|---|
| 0  | 0  | 0 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 0 |

### 隠れ層の設計
隠れ層は、入力層と出力層の中間に位置するニューラルネットワークの層です。  
隠れ層においては、隠れ層の数、各隠れ層におけるノードの数、各隠れ層における活性化関数を決める必要があります。  
隠れ層では、入力層と出力層と異なり、複数の層を設けることができます。

#### 隠れ層の数の決定
隠れ層の数を決めることは難しい問題です。  
殆どの場合、トライアンドエラーを通して良い隠れ層の数を見つける必要があります。  
ただ、一般的には、扱う問題の複雑性に比例して層を増やしていく事になります。  
それは、層を増すほど、ニューラルネットワークの表現力が高まるため、問題の複雑さに対しては層を厚くすることになります。  
しかし、層を増やしていくほど、学習に要する計算量が増え、また、表現力が高まることにより訓練データの特徴を過剰に表現し、汎化性能が低下する過学習を起こしやすくなります。  
また、最適なパラメータを推定する学習過程も難しくなる「勾配消失問題」と呼ばれる問題も出てきます。

#### 隠れ層のノード数の決定
隠れ層のノードの数を決めることは難しい問題です。  
殆どの場合、トライアンドエラーを通して良いノード数を見つける必要があります。  
ただ、一般的には、少なすぎるノード数は、過小学習を招き、訓練データに対する適合も低くなり、汎化性能も低くなります。  
また、多すぎるノード数は、過学習を招き、訓練データに対する適合は高いが、汎化性能が低くなってしまいます。  
経験則においては、入力層の入力数と出力層のノード数の中間がよいと言われています。  

#### 隠れ層の各ノードの重み・バイアスの決定
各隠れ層の各ノードにはそれぞれ重み、バイアスを持っています。  
この重み、バイアスを設定しないと、正しい推論はできません。  

#### 隠れ層の活性関数の選択
ニューラルネットワークでは、活性化関数に非線形関数を用いる必要があり、線形関数は使用できません。  
非線形関数とは、シグモイド関数やReLu関数のような曲線を含むなめらかな関数のことです。  
線形関数とはy=cxのような直線の関数になります。  

隠れ層において線形関数を使用すると、層を厚くしても、層を厚くする意味がなくなるためです。  
以下のコードでは、線形関数を活性化関数とする2層が1層と同じになることを示しています。  

```python
def linear_c2(x):
    c=2
    y = c*x
    return y

def linear_c4(x):
    c=4
    y = c*x
    return y

weighted_sum = 1
y1 = linear_c2(linear_c2(weighted_sum))
y2 = linear_c4(weighted_sum)

y1 == y2 == 4
```

このように、線形関数を隠れ層の活性化関数とすると、層を厚くするという隠れ層の利点が失われます。  

活性化関数としてどのようなものがあるか、下記を参照してください。  
[An overview of activation functions used in neural networks](https://adl1995.github.io/an-overview-of-activation-functions-used-in-neural-networks.html)

### 出力層の設計
出力層では、隠れ層からの出力を入力として、ニューラルネットワークの最終的な出力を生み出す層になります。  
出力層においては、出力層におけるノードの数、出力層における活性化関数を決める必要があります。  
また、ニューラルネットワークにおいては、必ず出力層が必要であり、全結合のネットワークの場合その数は必ず１つです。  

#### 出力層の各ノードの重み・バイアスの決定
出力層の各ノードにはそれぞれ重み、バイアスを持っています。  
この重み、バイアスを設定しないと、正しい推論はできません。  

#### 出力層のノードの数と活性化関数の選択
ノードの数と活性化関数の選択において、ニューラルネットワークが解く問題を意識する必要があります。  
機械学習の問題は、分類問題、回帰問題に大別され、分類問題とは男性、女性を判別するなどであり、回帰問題とは連続的な値を予測する問題です。

出力層のノードの数と活性化関数は、問題に応じて決める必要があります。  
多クラス分類問題の場合は、出力層のノードの数は分類先(クラスと呼ぶ)の数にし、活性化関数はソフトマックス関数を使用します  
１つのノードは１つのクラスに対応し、その出力はそのクラスである確率(0〜1)を意味します。  
例えば、画像を0〜9の数字に分類する場合、ノードの数は10になります。  
一つの画像に対して、あるノードがその画像が0である確率はいくつ、次のノードがその画像が1である確率はいくつと出力します。  

2値分類問題の場合は、出力層のノードの数は1つで、活性化関数はシグモイド関数などのなめらかな関数を使用します。  
ノードからの出力値は2値のどちらかになる確率を意味します。  
例えば、男女の2値分類では、ノードの出力が男性である確率を意味すれば、男性である確率が0の場合に女性であるとも解釈できるためです。  

回帰問題の場合は、出力層のノードの数は1つです。  
回帰問題の場合には、活性化関数として恒等関数が使われます。  
恒等関数とは、入力をそのまま出力する関数です。  

[パーセプトロンで論理回路を作成する](../perceptron)で扱った論理ゲートだと、2値分類問題になります。  
yは0か1の値しかとらず、4つすべての入力パターンを2値に分類する事になります。  
この場合、出力層のノードの数は1であり、活性化関数としてシグモイド関数などを利用します。  

#### XOR 真理値表
| x1 | x2 | y |
|----|----|---|
| 0  | 0  | 0 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 0 |

### XORゲートをKerasで書いてみる
上記を踏まえて、XORゲートのニューラルネットワークをKerasで定義してみましょう。  

```python
from keras.backend import relu
import tensorflow
from keras.models import Sequential
from keras.layers.core import Dense, Activation
import numpy as np

# ステップ関数
def step(x):
   x = tensorflow.sign ( tensorflow.sign( x ) + 0.1 )
   return relu(x, alpha=0., max_value=1, threshold=0.)

# XORゲートの2層ネットワークの定義
model = Sequential()

# 入力層および隠れ層
model.add(Dense(input_dim=2,                # 入力層のノード数
                units=2,                    # 隠れ層のノード数
                kernel_initializer='zeros', # 隠れ層の重み初期値
                bias_initializer='zeros',   # 隠れ層のバイアス初期値
                activation=step))           # 隠れ層の活性化関数

# 出力層
model.add(Dense(units=1,                    # 出力層のノード数
                kernel_initializer='zeros', # 出力層の重み初期値
                bias_initializer='zeros',   # 出力層のバイアス初期値
                activation=step))           # 出力層の活性化関数
```

XOR 真理値表から、入力層のノード数はx1, x2のふたつで2個になります。  
隠れ層のノード数はNANDゲートとORゲートを組み合わせたことから、2個になります。  
隠れ層は重み、バイアスを設定しますが、Kerasはニューラルネットワークを学習する(重み、バイアスを自動で獲得する)ことを前提とするため、個々のノードに重み、バイアスを設定することを前提にしていません。  
そのため、まず0初期化を行い、後に重み、バイアスを個別に設定します。  
隠れ層の活性化関数としてステップ関数を使用します(NANDゲート、ORゲートではステップ関数を利用)。  

出力層のノード数はANDゲートだったため、1個になります。  
出力層は重み、バイアスを設定しますが、Kerasはニューラルネットワークを学習する(重み、バイアスを自動で獲得する)ことを前提とするため、個々のノードに重み、バイアスを設定することを前提にしていません。  
そのため、まず0初期化を行い、後に重み、バイアスを個別に設定します。  
出力層の活性化関数としてステップ関数を使用します(XORゲートではステップ関数を利用)。  

隠れ層、出力層の活性化関数はステップ関数を利用していますが、Kerasでは以下の理由により提供されていません。  
そのため、ステップ関数を独自に実装しています。  

##### ニューラルネットワークの訓練では、ステップ関数を活性化関数として使用できない
ニューラルネットワークを訓練するには、損失関数の値を減らすような方法で入力に対する重みを調整します。  
勾配降下法においては、損失関数の値を減らすために、損失関数の値の勾配を計算し、損失関数が減少する方向にすこしずつ重みを変更していきます。  
ステップ関数は勾配を持たない関数です。  
このステップ関数を活性化関数に使用する層があると、勾配はすべて0になります。  
勾配がすべて0になるため、損失関数を減らすために重みをどの方向に移動させるかは決めることができません。  
これにより勾配をもたないステップ関数では、ニューラルネットワークの訓練ができません。  

#### XORゲートのテスト
作成したニューラルネットワークは、入力に対して出力を出す(順伝搬と呼ぶ)という意味では完成しています。  

##### ニューラルネットワークに対する順伝搬
では、作成したニューラルネットワークがXORゲートとして正しく振る舞えるか確認してみましょう。  

```python
x = np.array([[0,0],[0,1],[1,0],[1,1]]) # 入力データ
y_pred = model.predict(x) # 順伝搬
print('x1  x2  y:\n', np.hstack((x, y_pred)))
```

###### 実行結果

```bash
x1  x2  y:
 [[0. 0. 1.]
 [0. 1. 1.]
 [1. 0. 1.]
 [1. 1. 1.]]
```

XORゲートとして正しい論理演算結果とはなりませんでした。  
これは、ニューラルネットワークの重み、バイアスを0で初期化したため、正しい論理演算ができなかったためです。  

##### ニューラルネットワークの重み、バイアスを確認する。
ニューラルネットワークの重み、バイアスを確認してみましょう。  

```python
# 隠れ層の重みとバイアスを表示する。
hidden_layer = model.layers[0]
hidden_layer_weights = hidden_layer.get_weights()
print('Hidden Layer Weights:', np.array(hidden_layer.get_weights()[0]).flatten())
print('Hidden Layer Biases:', hidden_layer.get_weights()[1])

# 出力層の重みとバイアスを表示する。
output_layer = model.layers[1]
output_layer_weights = output_layer.get_weights()
print('Output Layer Weights:', np.array(output_layer.get_weights()[0]).flatten())
print('Output Layer Biases:', output_layer.get_weights()[1])
```

###### 実行結果
```python
Hidden Layer Weights: [0. 0. 0. 0.]
Hidden Layer Biases: [0. 0.]
Output Layer Weights: [0. 0.]
Output Layer Biases: [0.]
```

##### ニューラルネットワークの重み、バイアスを設定する。
XORゲートとして正しい推論を行うために、隠れ層、出力層の重み、バイアスを設定してみます。  

```python
# 隠れ層の重み、バイアスの設定
hidden_layer = model.layers[0]
hidden_layer_weights = hidden_layer.get_weights()

## NAND
## x1に対する重み： -0.5
## x2に対する重み： -0.5
## バイアス： 0.7
hidden_layer_weights[0][0][0] = -0.5
hidden_layer_weights[0][1][0] = -0.5
hidden_layer_weights[1][0] = 0.7

## OR
## x1に対する重み： 0.5
## x2に対する重み： 0.5
## バイアス： -0.2
hidden_layer_weights[0][0][1] = 0.5
hidden_layer_weights[0][1][1] = 0.5
hidden_layer_weights[1][1] = -0.2

hidden_layer.set_weights(hidden_layer_weights)

# 隠れ層の重みとバイアスを表示する。
print('Hidden Layer Weights:', np.array(hidden_layer.get_weights()[0]).flatten())
print('Hidden Layer Biases:', hidden_layer.get_weights()[1])


# 出力層の重み、バイアスの設定
output_layer = model.layers[1]
output_layer_weights = output_layer.get_weights()

## AND
## x1に対する重み： 0.5
## x2に対する重み： 0.5
## バイアス： -0.7
output_layer_weights[0][0][0] = 0.5
output_layer_weights[0][1][0] = 0.5
output_layer_weights[1][0] = -0.7

output_layer.set_weights(output_layer_weights)

# 出力層の重みとバイアスを表示する。
print('Output Layer Weights:', np.array(output_layer.get_weights()[0]).flatten())
print('Output Layer Biases:', output_layer.get_weights()[1])
```

###### 実行結果

```
Hidden Layer Weights: [-0.5  0.5 -0.5  0.5]
Hidden Layer Biases: [ 0.7 -0.2]
Output Layer Weights: [0.5 0.5]
Output Layer Biases: [-0.7]
```

##### ニューラルネットワークに対する順伝搬(再確認)
では、重み、バイアスを設定したニューラルネットワークがXORゲートとして正しく振る舞えるか確認してみましょう。  

```python
x = np.array([[0,0],[0,1],[1,0],[1,1]]) # 入力データ
y_pred = model.predict(x) # 順伝搬
print('x1  x2  y:\n', np.hstack((x, y_pred)))
```

###### 実行結果

```bash
x1  x2  y:
 [[0. 0. 0.]
 [0. 1. 1.]
 [1. 0. 1.]
 [1. 1. 0.]]
```

XORゲートとして正しい論理演算結果となりました。  

## Q. ADDゲートを実現するニューラルネットワークをKerasで作成してください。

#### ADDゲートの構成
|論理演算|x1の重み|x2の重み|バイアス|活性化関数  |対応層|
|--------|--------|--------|--------|------------|------|
|ADD     |1.0     |1.0     | 0.0    |恒等関数    |出力層|
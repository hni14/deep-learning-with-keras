# 論理回路の訓練を行う例

## 概要
[パーセプトロンで論理回路を作成する例](../perceptron) 題を通じて、パーセプトロンの仕組みを学びました。  
パーセプトロンの内部パラメータである、重みとバイアスは人が決めていましたが、ニューラルネットワーク(ディープラーニング)では、これらのパラメータは数千、数万とあり、人が決めていくことは不可能です。  
しかし、ニューラルネットワークでは、訓練データからパラメータを自動で獲得できる仕組みがあります。  
このパラメータを自動で獲得するプロセスをニューラルネットワークの学習と呼びます。  
この例題では、ニューラルネットワークの学習を扱います。  

この例題を通じて、以下の理解を目指します。  

- ニューラルネットワークの学習は、損失関数を指標として、損失関数の値が小さくなるようにパラメータを更新していく。
- パラメータを更新するために、重みの勾配を利用して、勾配方向に重みを少しずつ更新していく。
- 勾配を求めるために、誤差逆伝搬法を使用する。  

#### ニューラルネットワークではステップ関数が使用できない
ニューラルネットワークを訓練するには、エラー（または損失関数）を減らすような方法で入力に対する重みを調整します。  
勾配降下法においては、損失関数を減らすために、活性化関数の勾配を計算し、損失関数が減少する方向にすこしずつ重みを変更していきます。  
しかし、ステップ関数を使用すると、勾配はすべて0になります。  
勾配がすべて0になるため、損失関数を減らすために重みをどの方向に移動させるかは決めることができません。  
これにより勾配をもたないステップ関数では、ニューラルネットワークの訓練ができません。  

### ネットワーク構造の決定

#### 入力層

#### 隠れ層

##### 隠れ層の数

##### 隠れ層のパーセプトロンの数

##### 隠れ層の活性関数の選択

#### 出力層

##### 出力層の活性関数の選択

### 学習方法の決定

#### 重み、バイアスの初期値

#### 損失関数

#### 最適化

#### 誤差逆伝搬における学習率

#### 学習時のエポック数、ミニバッチサイズ
